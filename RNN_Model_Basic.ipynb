{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bduh-46OHFK",
   "metadata": {
    "id": "3bduh-46OHFK"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iUFDKHaVOHFN",
   "metadata": {
    "id": "iUFDKHaVOHFN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beLBhgRLOHFP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beLBhgRLOHFP",
    "outputId": "fe017141-8c90-4f18-d260-b91583d3e971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   _unit_id               8000 non-null   int64  \n",
      " 1   _golden                8000 non-null   bool   \n",
      " 2   _unit_state            8000 non-null   object \n",
      " 3   _trusted_judgments     8000 non-null   int64  \n",
      " 4   _last_judgment_at      8000 non-null   object \n",
      " 5   positivity             1420 non-null   float64\n",
      " 6   positivity:confidence  3775 non-null   float64\n",
      " 7   relevance              8000 non-null   object \n",
      " 8   relevance:confidence   8000 non-null   float64\n",
      " 9   articleid              8000 non-null   object \n",
      " 10  date                   8000 non-null   object \n",
      " 11  headline               8000 non-null   object \n",
      " 12  positivity_gold        0 non-null      float64\n",
      " 13  relevance_gold         0 non-null      float64\n",
      " 14  text                   8000 non-null   object \n",
      "dtypes: bool(1), float64(5), int64(2), object(7)\n",
      "memory usage: 882.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./US-Economic-News.csv\", delimiter=',', encoding= 'ISO-8859-1')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2pZ91v4YOHFQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "2pZ91v4YOHFQ",
    "outputId": "2ecb5bf5-5337-4d44-bf14-f45d0725524e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>positivity_gold</th>\n",
       "      <th>relevance_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842613455</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 17:48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398217788</td>\n",
       "      <td>8/14/91</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842613456</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 16:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_399019502</td>\n",
       "      <td>8/21/07</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>842613457</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 1:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_398284048</td>\n",
       "      <td>11/14/91</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>842613458</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 2:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>no</td>\n",
       "      <td>0.675</td>\n",
       "      <td>wsj_397959018</td>\n",
       "      <td>6/16/86</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>842613459</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/5/15 17:48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398838054</td>\n",
       "      <td>10/4/02</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  842613455    False   finalized                   3     12/5/15 17:48   \n",
       "1  842613456    False   finalized                   3     12/5/15 16:54   \n",
       "2  842613457    False   finalized                   3      12/5/15 1:59   \n",
       "3  842613458    False   finalized                   3      12/5/15 2:19   \n",
       "4  842613459    False   finalized                   3     12/5/15 17:48   \n",
       "\n",
       "   positivity  positivity:confidence relevance  relevance:confidence  \\\n",
       "0         3.0                 0.6400       yes                 0.640   \n",
       "1         NaN                    NaN        no                 1.000   \n",
       "2         NaN                    NaN        no                 1.000   \n",
       "3         NaN                 0.0000        no                 0.675   \n",
       "4         3.0                 0.3257       yes                 0.640   \n",
       "\n",
       "       articleid      date                                           headline  \\\n",
       "0  wsj_398217788   8/14/91              Yields on CDs Fell in the Latest Week   \n",
       "1  wsj_399019502   8/21/07  The Morning Brief: White House Seeks to Limit ...   \n",
       "2  wsj_398284048  11/14/91  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3  wsj_397959018   6/16/86  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4  wsj_398838054   10/4/02  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "\n",
       "   positivity_gold  relevance_gold  \\\n",
       "0              NaN             NaN   \n",
       "1              NaN             NaN   \n",
       "2              NaN             NaN   \n",
       "3              NaN             NaN   \n",
       "4              NaN             NaN   \n",
       "\n",
       "                                                text  \n",
       "0  NEW YORK -- Yields on most certificates of dep...  \n",
       "1  The Wall Street Journal Online</br></br>The Mo...  \n",
       "2  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3  The statistics on the enormous costs of employ...  \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfFHenMOHFQ",
   "metadata": {
    "id": "6dfFHenMOHFQ"
   },
   "outputs": [],
   "source": [
    "df = df[['headline', 'text', 'relevance']]\n",
    "\n",
    "# We drop all irrelavant features to only keep headline and text for 2 reasons: \n",
    "# The other features seem either irrelevant or we lack documentation\n",
    "# With headline and text only, our final model will be more generalizable. We could in theory apply it to any article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "oRrzx8WvOHFR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oRrzx8WvOHFR",
    "outputId": "7b9c6dd5-8a57-47a0-9c63-72bac3ab1bea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0              Yields on CDs Fell in the Latest Week   \n",
       "1  The Morning Brief: White House Seeks to Limit ...   \n",
       "2  Banking Bill Negotiators Set Compromise --- Pl...   \n",
       "3  Manager's Journal: Sniffing Out Drug Abusers I...   \n",
       "4  Currency Trading: Dollar Remains in Tight Rang...   \n",
       "\n",
       "                                                text relevance  \n",
       "0  NEW YORK -- Yields on most certificates of dep...       yes  \n",
       "1  The Wall Street Journal Online</br></br>The Mo...        no  \n",
       "2  WASHINGTON -- In an effort to achieve banking ...        no  \n",
       "3  The statistics on the enormous costs of employ...        no  \n",
       "4  NEW YORK -- Indecision marked the dollar's ton...       yes  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jK-dXnQAOHFR",
   "metadata": {
    "id": "jK-dXnQAOHFR"
   },
   "source": [
    "Cleaning Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "kS0pN2EEOHFS",
   "metadata": {
    "id": "kS0pN2EEOHFS"
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fpS1zR46OHFS",
   "metadata": {
    "id": "fpS1zR46OHFS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "T8bYfqW0OHFS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8bYfqW0OHFS",
    "outputId": "0e457b24-c049-4984-ed6b-29f344f613b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\majon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\majon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\majon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Ensure you have downloaded the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hwsu0fkMOHFS",
   "metadata": {
    "id": "hwsu0fkMOHFS"
   },
   "outputs": [],
   "source": [
    "df['whole_txt'] = df['headline']+ ' ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tGK5t8RyOHFT",
   "metadata": {
    "id": "tGK5t8RyOHFT"
   },
   "outputs": [],
   "source": [
    "wtxt_train = np.array(df['whole_txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "kPJu-IuxOHFT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPJu-IuxOHFT",
    "outputId": "38626369-0963-40be-8d89-321e151d7b9b"
   },
   "outputs": [],
   "source": [
    "#print(wtxt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8EX1KU3FOHFT",
   "metadata": {
    "id": "8EX1KU3FOHFT"
   },
   "outputs": [],
   "source": [
    "for i in range(len(wtxt_train)):\n",
    "    # Taking out '<br>' in the 'whole_text' column\n",
    "    wtxt_train[i] = re.sub(r'</?br>', ' ', wtxt_train[i])\n",
    "    # Deletion of non-latin alfabet signs, also numbers\n",
    "    wtxt_train[i] = re.sub(r'[^a-zA-Z]', ' ', wtxt_train[i])\n",
    "    # Removing single letter works like 'a'.\n",
    "    wtxt_train[i] = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', wtxt_train[i])\n",
    "    # Removing double spaces\n",
    "    wtxt_train[i] = re.sub(r'\\s+', ' ', wtxt_train[i])\n",
    "    # Lower case\n",
    "    wtxt_train[i] = wtxt_train[i].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OGok8jBiOHFT",
   "metadata": {
    "id": "OGok8jBiOHFT"
   },
   "source": [
    "Split the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "YLC_hbBPOHFT",
   "metadata": {
    "id": "YLC_hbBPOHFT"
   },
   "outputs": [],
   "source": [
    "for i in range(len(wtxt_train)):\n",
    "    wtxt_train[i] = word_tokenize(wtxt_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nJcjnqLHOHFT",
   "metadata": {
    "id": "nJcjnqLHOHFT"
   },
   "source": [
    "Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7uoaH6INOHFT",
   "metadata": {
    "id": "7uoaH6INOHFT"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(wtxt_train)):\n",
    "    wtxt_train[i] = [word for word in wtxt_train[i] if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74Ii4vxnOHFU",
   "metadata": {
    "id": "74Ii4vxnOHFU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yields',\n",
       " 'cds',\n",
       " 'fell',\n",
       " 'latest',\n",
       " 'week',\n",
       " 'new',\n",
       " 'york',\n",
       " 'yields',\n",
       " 'certificates',\n",
       " 'deposit',\n",
       " 'offered',\n",
       " 'major',\n",
       " 'banks',\n",
       " 'dropped',\n",
       " 'tenth',\n",
       " 'percentage',\n",
       " 'point',\n",
       " 'latest',\n",
       " 'week',\n",
       " 'reflecting',\n",
       " 'overall',\n",
       " 'decline',\n",
       " 'short',\n",
       " 'term',\n",
       " 'interest',\n",
       " 'rates',\n",
       " 'small',\n",
       " 'denomination',\n",
       " 'consumer',\n",
       " 'cds',\n",
       " 'sold',\n",
       " 'directly',\n",
       " 'banks',\n",
       " 'average',\n",
       " 'yield',\n",
       " 'six',\n",
       " 'month',\n",
       " 'deposits',\n",
       " 'fell',\n",
       " 'week',\n",
       " 'ended',\n",
       " 'yesterday',\n",
       " 'according',\n",
       " 'bank',\n",
       " 'survey',\n",
       " 'banxquote',\n",
       " 'money',\n",
       " 'markets',\n",
       " 'wilmington',\n",
       " 'del',\n",
       " 'information',\n",
       " 'service',\n",
       " 'three',\n",
       " 'month',\n",
       " 'consumer',\n",
       " 'deposits',\n",
       " 'average',\n",
       " 'yield',\n",
       " 'sank',\n",
       " 'week',\n",
       " 'according',\n",
       " 'banxquote',\n",
       " 'two',\n",
       " 'banks',\n",
       " 'banxquote',\n",
       " 'survey',\n",
       " 'citibank',\n",
       " 'new',\n",
       " 'york',\n",
       " 'corestates',\n",
       " 'pennsylvania',\n",
       " 'paying',\n",
       " 'less',\n",
       " 'threemonth',\n",
       " 'small',\n",
       " 'denomination',\n",
       " 'cds',\n",
       " 'declines',\n",
       " 'somewhat',\n",
       " 'smaller',\n",
       " 'five',\n",
       " 'year',\n",
       " 'consumer',\n",
       " 'cds',\n",
       " 'eased',\n",
       " 'banxquote',\n",
       " 'said',\n",
       " 'yields',\n",
       " 'three',\n",
       " 'month',\n",
       " 'six',\n",
       " 'month',\n",
       " 'treasury',\n",
       " 'bills',\n",
       " 'sold',\n",
       " 'monday',\n",
       " 'auction',\n",
       " 'plummeted',\n",
       " 'fifth',\n",
       " 'percentage',\n",
       " 'point',\n",
       " 'previous',\n",
       " 'week',\n",
       " 'respectively']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wtxt_train[0]\n",
    "# stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Ih9nJCDOHFU",
   "metadata": {
    "id": "0Ih9nJCDOHFU"
   },
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tZq0soQFOHFU",
   "metadata": {
    "id": "tZq0soQFOHFU"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(wtxt_train)):\n",
    "    wtxt_train[i] = [lemmatizer.lemmatize(word) for word in wtxt_train[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "yh7EcIK7OHFU",
   "metadata": {
    "id": "yh7EcIK7OHFU"
   },
   "outputs": [],
   "source": [
    "df['whole_txt'] = wtxt_train\n",
    "df = df.drop(['headline', 'text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "T1TLLZZhOHFU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "T1TLLZZhOHFU",
    "outputId": "127d4d92-3e49-4449-8bff-a5ab1fa375cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>whole_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>[yield, cd, fell, latest, week, new, york, yie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>[morning, brief, white, house, seek, limit, ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>[banking, bill, negotiator, set, compromise, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>[manager, journal, sniffing, drug, abuser, qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>[currency, trading, dollar, remains, tight, ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relevance                                          whole_txt\n",
       "0       yes  [yield, cd, fell, latest, week, new, york, yie...\n",
       "1        no  [morning, brief, white, house, seek, limit, ch...\n",
       "2        no  [banking, bill, negotiator, set, compromise, p...\n",
       "3        no  [manager, journal, sniffing, drug, abuser, qui...\n",
       "4       yes  [currency, trading, dollar, remains, tight, ra..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1e868e34",
   "metadata": {
    "id": "1e868e34"
   },
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ecc91",
   "metadata": {
    "id": "2a9ecc91"
   },
   "source": [
    "### Data preparation\n",
    "* Initial Data Processing: Our first step is to encode the relevance label into both the Relevant (1) and non-Relevant labels (0). Then, we make it into a np.array to feed into the model.\n",
    "* Then, we begin to clean text data into pad sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d046d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.update(df[\"relevance\"].apply(lambda x: 0 if x == \"no\" else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c64fe00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>whole_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[yield, cd, fell, latest, week, new, york, yie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[morning, brief, white, house, seek, limit, ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[banking, bill, negotiator, set, compromise, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[manager, journal, sniffing, drug, abuser, qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[currency, trading, dollar, remains, tight, ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relevance                                          whole_txt\n",
       "0         1  [yield, cd, fell, latest, week, new, york, yie...\n",
       "1         0  [morning, brief, white, house, seek, limit, ch...\n",
       "2         0  [banking, bill, negotiator, set, compromise, p...\n",
       "3         0  [manager, journal, sniffing, drug, abuser, qui...\n",
       "4         1  [currency, trading, dollar, remains, tight, ra..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc686094",
   "metadata": {
    "id": "bc686094"
   },
   "source": [
    "### Tokenization\n",
    "First, we need to \"tokenize\" our sentences, i.e., convert them to sequences of numbers. For this task, we are going to use the `Tokenizer` from Tensorflow (documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf9eaddf",
   "metadata": {
    "id": "cf9eaddf"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wtxt_train)   # fit our tokenizer on the dataset (i.e., assign a number to each word and keep a\n",
    "                                    # dictionary with the correspondence of each word to a number)\n",
    "\n",
    "# see the language dictionary and the total number of words (please note that number 0 is reserved for the padding task)\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bb124ce",
   "metadata": {
    "id": "1bb124ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 1,\n",
       " 'market': 2,\n",
       " 'said': 3,\n",
       " 'rate': 4,\n",
       " 'stock': 5,\n",
       " 'new': 6,\n",
       " 'price': 7,\n",
       " 'would': 8,\n",
       " 'economy': 9,\n",
       " 'bank': 10,\n",
       " 'economic': 11,\n",
       " 'percent': 12,\n",
       " 'interest': 13,\n",
       " 'federal': 14,\n",
       " 'last': 15,\n",
       " 'month': 16,\n",
       " 'company': 17,\n",
       " 'billion': 18,\n",
       " 'week': 19,\n",
       " 'one': 20,\n",
       " 'inflation': 21,\n",
       " 'million': 22,\n",
       " 'investor': 23,\n",
       " 'fed': 24,\n",
       " 'point': 25,\n",
       " 'dollar': 26,\n",
       " 'time': 27,\n",
       " 'bond': 28,\n",
       " 'tax': 29,\n",
       " 'government': 30,\n",
       " 'president': 31,\n",
       " 'first': 32,\n",
       " 'york': 33,\n",
       " 'say': 34,\n",
       " 'growth': 35,\n",
       " 'fund': 36,\n",
       " 'day': 37,\n",
       " 'increase': 38,\n",
       " 'yesterday': 39,\n",
       " 'reserve': 40,\n",
       " 'share': 41,\n",
       " 'also': 42,\n",
       " 'business': 43,\n",
       " 'average': 44,\n",
       " 'since': 45,\n",
       " 'two': 46,\n",
       " 'may': 47,\n",
       " 'many': 48,\n",
       " 'state': 49,\n",
       " 'index': 50,\n",
       " 'could': 51,\n",
       " 'high': 52,\n",
       " 'quarter': 53,\n",
       " 'money': 54,\n",
       " 'report': 55,\n",
       " 'mr': 56,\n",
       " 'job': 57,\n",
       " 'trading': 58,\n",
       " 'cut': 59,\n",
       " 'financial': 60,\n",
       " 'rose': 61,\n",
       " 'deficit': 62,\n",
       " 'consumer': 63,\n",
       " 'sale': 64,\n",
       " 'gain': 65,\n",
       " 'higher': 66,\n",
       " 'dow': 67,\n",
       " 'policy': 68,\n",
       " 'even': 69,\n",
       " 'budget': 70,\n",
       " 'economist': 71,\n",
       " 'american': 72,\n",
       " 'analyst': 73,\n",
       " 'much': 74,\n",
       " 'term': 75,\n",
       " 'trade': 76,\n",
       " 'treasury': 77,\n",
       " 'today': 78,\n",
       " 'spending': 79,\n",
       " 'people': 80,\n",
       " 'recession': 81,\n",
       " 'cost': 82,\n",
       " 'security': 83,\n",
       " 'rise': 84,\n",
       " 'plan': 85,\n",
       " 'decline': 86,\n",
       " 'house': 87,\n",
       " 'fell': 88,\n",
       " 'washington': 89,\n",
       " 'good': 90,\n",
       " 'level': 91,\n",
       " 'cent': 92,\n",
       " 'low': 93,\n",
       " 'three': 94,\n",
       " 'official': 95,\n",
       " 'exchange': 96,\n",
       " 'unemployment': 97,\n",
       " 'investment': 98,\n",
       " 'nation': 99,\n",
       " 'home': 100,\n",
       " 'long': 101,\n",
       " 'world': 102,\n",
       " 'make': 103,\n",
       " 'lower': 104,\n",
       " 'credit': 105,\n",
       " 'loan': 106,\n",
       " 'still': 107,\n",
       " 'department': 108,\n",
       " 'industry': 109,\n",
       " 'issue': 110,\n",
       " 'expected': 111,\n",
       " 'country': 112,\n",
       " 'industrial': 113,\n",
       " 'next': 114,\n",
       " 'recent': 115,\n",
       " 'back': 116,\n",
       " 'board': 117,\n",
       " 'end': 118,\n",
       " 'big': 119,\n",
       " 'chairman': 120,\n",
       " 'income': 121,\n",
       " 'record': 122,\n",
       " 'past': 123,\n",
       " 'labor': 124,\n",
       " 'news': 125,\n",
       " 'firm': 126,\n",
       " 'number': 127,\n",
       " 'service': 128,\n",
       " 'program': 129,\n",
       " 'jones': 130,\n",
       " 'profit': 131,\n",
       " 'administration': 132,\n",
       " 'according': 133,\n",
       " 'way': 134,\n",
       " 'street': 135,\n",
       " 'group': 136,\n",
       " 'national': 137,\n",
       " 'mortgage': 138,\n",
       " 'le': 139,\n",
       " 'well': 140,\n",
       " 'get': 141,\n",
       " 'like': 142,\n",
       " 'oil': 143,\n",
       " 'worker': 144,\n",
       " 'per': 145,\n",
       " 'bill': 146,\n",
       " 'major': 147,\n",
       " 'inc': 148,\n",
       " 'work': 149,\n",
       " 'loss': 150,\n",
       " 'second': 151,\n",
       " 'debt': 152,\n",
       " 'wall': 153,\n",
       " 'chief': 154,\n",
       " 'late': 155,\n",
       " 'short': 156,\n",
       " 'congress': 157,\n",
       " 'currency': 158,\n",
       " 'move': 159,\n",
       " 'change': 160,\n",
       " 'pay': 161,\n",
       " 'take': 162,\n",
       " 'system': 163,\n",
       " 'strong': 164,\n",
       " 'early': 165,\n",
       " 'future': 166,\n",
       " 'recovery': 167,\n",
       " 'another': 168,\n",
       " 'made': 169,\n",
       " 'part': 170,\n",
       " 'data': 171,\n",
       " 'close': 172,\n",
       " 'fall': 173,\n",
       " 'bush': 174,\n",
       " 'reported': 175,\n",
       " 'problem': 176,\n",
       " 'central': 177,\n",
       " 'capital': 178,\n",
       " 'earlier': 179,\n",
       " 'drop': 180,\n",
       " 'corp': 181,\n",
       " 'come': 182,\n",
       " 'co': 183,\n",
       " 'friday': 184,\n",
       " 'earnings': 185,\n",
       " 'yield': 186,\n",
       " 'among': 187,\n",
       " 'public': 188,\n",
       " 'rising': 189,\n",
       " 'see': 190,\n",
       " 'real': 191,\n",
       " 'nasdaq': 192,\n",
       " 'small': 193,\n",
       " 'little': 194,\n",
       " 'figure': 195,\n",
       " 'committee': 196,\n",
       " 'foreign': 197,\n",
       " 'third': 198,\n",
       " 'executive': 199,\n",
       " 'result': 200,\n",
       " 'ago': 201,\n",
       " 'raise': 202,\n",
       " 'help': 203,\n",
       " 'annual': 204,\n",
       " 'half': 205,\n",
       " 'nearly': 206,\n",
       " 'u': 207,\n",
       " 'go': 208,\n",
       " 'far': 209,\n",
       " 'value': 210,\n",
       " 'show': 211,\n",
       " 'based': 212,\n",
       " 'likely': 213,\n",
       " 'demand': 214,\n",
       " 'put': 215,\n",
       " 'city': 216,\n",
       " 'standard': 217,\n",
       " 'however': 218,\n",
       " 'global': 219,\n",
       " 'might': 220,\n",
       " 'june': 221,\n",
       " 'july': 222,\n",
       " 'product': 223,\n",
       " 'set': 224,\n",
       " 'keep': 225,\n",
       " 'international': 226,\n",
       " 'rally': 227,\n",
       " 'need': 228,\n",
       " 'sign': 229,\n",
       " 'four': 230,\n",
       " 'current': 231,\n",
       " 'trader': 232,\n",
       " 'japan': 233,\n",
       " 'housing': 234,\n",
       " 'area': 235,\n",
       " 'concern': 236,\n",
       " 'march': 237,\n",
       " 'buy': 238,\n",
       " 'meeting': 239,\n",
       " 'united': 240,\n",
       " 'office': 241,\n",
       " 'corporate': 242,\n",
       " 'benefit': 243,\n",
       " 'crisis': 244,\n",
       " 'better': 245,\n",
       " 'fiscal': 246,\n",
       " 'five': 247,\n",
       " 'large': 248,\n",
       " 'january': 249,\n",
       " 'session': 250,\n",
       " 'including': 251,\n",
       " 'deal': 252,\n",
       " 'several': 253,\n",
       " 'member': 254,\n",
       " 'going': 255,\n",
       " 'period': 256,\n",
       " 'measure': 257,\n",
       " 'general': 258,\n",
       " 'risk': 259,\n",
       " 'think': 260,\n",
       " 'april': 261,\n",
       " 'poor': 262,\n",
       " 'white': 263,\n",
       " 'political': 264,\n",
       " 'manager': 265,\n",
       " 'right': 266,\n",
       " 'largest': 267,\n",
       " 'volume': 268,\n",
       " 'america': 269,\n",
       " 'tuesday': 270,\n",
       " 'hit': 271,\n",
       " 'senate': 272,\n",
       " 'despite': 273,\n",
       " 'fear': 274,\n",
       " 'revenue': 275,\n",
       " 'health': 276,\n",
       " 'around': 277,\n",
       " 'republican': 278,\n",
       " 'start': 279,\n",
       " 'increased': 280,\n",
       " 'least': 281,\n",
       " 'compared': 282,\n",
       " 'sector': 283,\n",
       " 'thing': 284,\n",
       " 'whether': 285,\n",
       " 'greenspan': 286,\n",
       " 'buying': 287,\n",
       " 'already': 288,\n",
       " 'monday': 289,\n",
       " 'finance': 290,\n",
       " 'continued': 291,\n",
       " 'enough': 292,\n",
       " 'six': 293,\n",
       " 'ahead': 294,\n",
       " 'pressure': 295,\n",
       " 'though': 296,\n",
       " 'forecast': 297,\n",
       " 'top': 298,\n",
       " 'wage': 299,\n",
       " 'want': 300,\n",
       " 'employment': 301,\n",
       " 'return': 302,\n",
       " 'biggest': 303,\n",
       " 'sell': 304,\n",
       " 'monetary': 305,\n",
       " 'china': 306,\n",
       " 'order': 307,\n",
       " 'mean': 308,\n",
       " 'look': 309,\n",
       " 'technology': 310,\n",
       " 'called': 311,\n",
       " 'yen': 312,\n",
       " 'private': 313,\n",
       " 'came': 314,\n",
       " 'without': 315,\n",
       " 'wednesday': 316,\n",
       " 'making': 317,\n",
       " 'growing': 318,\n",
       " 'effort': 319,\n",
       " 'action': 320,\n",
       " 'ended': 321,\n",
       " 'best': 322,\n",
       " 'continue': 323,\n",
       " 'law': 324,\n",
       " 'run': 325,\n",
       " 'leader': 326,\n",
       " 'management': 327,\n",
       " 'began': 328,\n",
       " 'closed': 329,\n",
       " 'thursday': 330,\n",
       " 'clinton': 331,\n",
       " 'proposal': 332,\n",
       " 'war': 333,\n",
       " 'insurance': 334,\n",
       " 'force': 335,\n",
       " 'call': 336,\n",
       " 'added': 337,\n",
       " 'post': 338,\n",
       " 'total': 339,\n",
       " 'reagan': 340,\n",
       " 'although': 341,\n",
       " 'mark': 342,\n",
       " 'selling': 343,\n",
       " 'almost': 344,\n",
       " 'yet': 345,\n",
       " 'latest': 346,\n",
       " 'face': 347,\n",
       " 'saving': 348,\n",
       " 'survey': 349,\n",
       " 'county': 350,\n",
       " 'advance': 351,\n",
       " 'effect': 352,\n",
       " 'asset': 353,\n",
       " 'lost': 354,\n",
       " 'support': 355,\n",
       " 'sharply': 356,\n",
       " 'employee': 357,\n",
       " 'question': 358,\n",
       " 'director': 359,\n",
       " 'old': 360,\n",
       " 'hour': 361,\n",
       " 'account': 362,\n",
       " 'control': 363,\n",
       " 'supply': 364,\n",
       " 'boost': 365,\n",
       " 'cash': 366,\n",
       " 'every': 367,\n",
       " 'give': 368,\n",
       " 'european': 369,\n",
       " 'percentage': 370,\n",
       " 'become': 371,\n",
       " 'union': 372,\n",
       " 'dropped': 373,\n",
       " 'pace': 374,\n",
       " 'commerce': 375,\n",
       " 'offer': 376,\n",
       " 'europe': 377,\n",
       " 'taking': 378,\n",
       " 'banking': 379,\n",
       " 'activity': 380,\n",
       " 'previous': 381,\n",
       " 'energy': 382,\n",
       " 'decade': 383,\n",
       " 'told': 384,\n",
       " 'october': 385,\n",
       " 'social': 386,\n",
       " 'power': 387,\n",
       " 'line': 388,\n",
       " 'took': 389,\n",
       " 'case': 390,\n",
       " 'japanese': 391,\n",
       " 'maker': 392,\n",
       " 'democrat': 393,\n",
       " 'declined': 394,\n",
       " 'school': 395,\n",
       " 'export': 396,\n",
       " 'estimate': 397,\n",
       " 'showed': 398,\n",
       " 'gold': 399,\n",
       " 'domestic': 400,\n",
       " 'key': 401,\n",
       " 'december': 402,\n",
       " 'slightly': 403,\n",
       " 'august': 404,\n",
       " 'hope': 405,\n",
       " 'secretary': 406,\n",
       " 'care': 407,\n",
       " 'euro': 408,\n",
       " 'fourth': 409,\n",
       " 'composite': 410,\n",
       " 'rule': 411,\n",
       " 'family': 412,\n",
       " 'note': 413,\n",
       " 'agency': 414,\n",
       " 'food': 415,\n",
       " 'november': 416,\n",
       " 'production': 417,\n",
       " 'research': 418,\n",
       " 'blue': 419,\n",
       " 'september': 420,\n",
       " 'expect': 421,\n",
       " 'decision': 422,\n",
       " 'must': 423,\n",
       " 'chip': 424,\n",
       " 'expectation': 425,\n",
       " 'amount': 426,\n",
       " 'net': 427,\n",
       " 'association': 428,\n",
       " 'coming': 429,\n",
       " 'find': 430,\n",
       " 'highest': 431,\n",
       " 'view': 432,\n",
       " 'auto': 433,\n",
       " 'reason': 434,\n",
       " 'outlook': 435,\n",
       " 'local': 436,\n",
       " 'place': 437,\n",
       " 'open': 438,\n",
       " 'former': 439,\n",
       " 'february': 440,\n",
       " 'later': 441,\n",
       " 'productivity': 442,\n",
       " 'lowest': 443,\n",
       " 'contract': 444,\n",
       " 'retail': 445,\n",
       " 'head': 446,\n",
       " 'payment': 447,\n",
       " 'vice': 448,\n",
       " 'full': 449,\n",
       " 'near': 450,\n",
       " 'estate': 451,\n",
       " 'soon': 452,\n",
       " 'know': 453,\n",
       " 'campaign': 454,\n",
       " 'option': 455,\n",
       " 'hold': 456,\n",
       " 'steel': 457,\n",
       " 'fact': 458,\n",
       " 'commission': 459,\n",
       " 'important': 460,\n",
       " 'election': 461,\n",
       " 'seen': 462,\n",
       " 'lot': 463,\n",
       " 'sharp': 464,\n",
       " 'bad': 465,\n",
       " 'lead': 466,\n",
       " 'unit': 467,\n",
       " 'announced': 468,\n",
       " 'university': 469,\n",
       " 'used': 470,\n",
       " 'worry': 471,\n",
       " 'released': 472,\n",
       " 'computer': 473,\n",
       " 'recently': 474,\n",
       " 'believe': 475,\n",
       " 'institution': 476,\n",
       " 'helped': 477,\n",
       " 'toward': 478,\n",
       " 'performance': 479,\n",
       " 'equity': 480,\n",
       " 'store': 481,\n",
       " 'left': 482,\n",
       " 'car': 483,\n",
       " 'charge': 484,\n",
       " 'life': 485,\n",
       " 'reduce': 486,\n",
       " 'weak': 487,\n",
       " 'falling': 488,\n",
       " 'away': 489,\n",
       " 'confidence': 490,\n",
       " 'manufacturing': 491,\n",
       " 'congressional': 492,\n",
       " 'summer': 493,\n",
       " 'use': 494,\n",
       " 'district': 495,\n",
       " 'led': 496,\n",
       " 'hard': 497,\n",
       " 'purchase': 498,\n",
       " 'longer': 499,\n",
       " 'turn': 500,\n",
       " 'slow': 501,\n",
       " 'study': 502,\n",
       " 'talk': 503,\n",
       " 'card': 504,\n",
       " 'senior': 505,\n",
       " 'jobless': 506,\n",
       " 'holding': 507,\n",
       " 'overall': 508,\n",
       " 'held': 509,\n",
       " 'indicator': 510,\n",
       " 'import': 511,\n",
       " 'great': 512,\n",
       " 'following': 513,\n",
       " 'vote': 514,\n",
       " 'council': 515,\n",
       " 'statement': 516,\n",
       " 'seven': 517,\n",
       " 'reduction': 518,\n",
       " 'often': 519,\n",
       " 'amid': 520,\n",
       " 'robert': 521,\n",
       " 'individual': 522,\n",
       " 'target': 523,\n",
       " 'democratic': 524,\n",
       " 'industrials': 525,\n",
       " 'defense': 526,\n",
       " 'remain': 527,\n",
       " 'looking': 528,\n",
       " 'hand': 529,\n",
       " 'p': 530,\n",
       " 'john': 531,\n",
       " 'banker': 532,\n",
       " 'found': 533,\n",
       " 'leading': 534,\n",
       " 'development': 535,\n",
       " 'instead': 536,\n",
       " 'trend': 537,\n",
       " 'strength': 538,\n",
       " 'party': 539,\n",
       " 'course': 540,\n",
       " 'expansion': 541,\n",
       " 'step': 542,\n",
       " 'raising': 543,\n",
       " 'lending': 544,\n",
       " 'mixed': 545,\n",
       " 'rather': 546,\n",
       " 'mutual': 547,\n",
       " 'personal': 548,\n",
       " 'agreement': 549,\n",
       " 'gained': 550,\n",
       " 'monthly': 551,\n",
       " 'gross': 552,\n",
       " 'others': 553,\n",
       " 'due': 554,\n",
       " 'conference': 555,\n",
       " 'buyer': 556,\n",
       " 'adjusted': 557,\n",
       " 'push': 558,\n",
       " 'surge': 559,\n",
       " 'position': 560,\n",
       " 'customer': 561,\n",
       " 'obama': 562,\n",
       " 'region': 563,\n",
       " 'building': 564,\n",
       " 'sold': 565,\n",
       " 'impact': 566,\n",
       " 'factor': 567,\n",
       " 'raised': 568,\n",
       " 'begin': 569,\n",
       " 'provide': 570,\n",
       " 'fixed': 571,\n",
       " 'operation': 572,\n",
       " 'proposed': 573,\n",
       " 'aid': 574,\n",
       " 'condition': 575,\n",
       " 'getting': 576,\n",
       " 'within': 577,\n",
       " 'free': 578,\n",
       " 'output': 579,\n",
       " 'got': 580,\n",
       " 'traded': 581,\n",
       " 'black': 582,\n",
       " 'possible': 583,\n",
       " 'known': 584,\n",
       " 'example': 585,\n",
       " 'list': 586,\n",
       " 'given': 587,\n",
       " 'claim': 588,\n",
       " 'west': 589,\n",
       " 'balance': 590,\n",
       " 'ever': 591,\n",
       " 'regulator': 592,\n",
       " 'evidence': 593,\n",
       " 'retailer': 594,\n",
       " 'lender': 595,\n",
       " 'along': 596,\n",
       " 'motor': 597,\n",
       " 'especially': 598,\n",
       " 'history': 599,\n",
       " 'center': 600,\n",
       " 'comment': 601,\n",
       " 'offering': 602,\n",
       " 'across': 603,\n",
       " 'strategy': 604,\n",
       " 'slowdown': 605,\n",
       " 'final': 606,\n",
       " 'seems': 607,\n",
       " 'reform': 608,\n",
       " 'commodity': 609,\n",
       " 'range': 610,\n",
       " 'alan': 611,\n",
       " 'bernanke': 612,\n",
       " 'probably': 613,\n",
       " 'remains': 614,\n",
       " 'trillion': 615,\n",
       " 'mid': 616,\n",
       " 'reached': 617,\n",
       " 'matter': 618,\n",
       " 'meet': 619,\n",
       " 'college': 620,\n",
       " 'c': 621,\n",
       " 'taken': 622,\n",
       " 'living': 623,\n",
       " 'broad': 624,\n",
       " 'act': 625,\n",
       " 'idea': 626,\n",
       " 'generally': 627,\n",
       " 'discount': 628,\n",
       " 'middle': 629,\n",
       " 'portfolio': 630,\n",
       " 'information': 631,\n",
       " 'significant': 632,\n",
       " 'side': 633,\n",
       " 'prospect': 634,\n",
       " 'adviser': 635,\n",
       " 'clear': 636,\n",
       " 'press': 637,\n",
       " 'limit': 638,\n",
       " 'community': 639,\n",
       " 'fee': 640,\n",
       " 'heavy': 641,\n",
       " 'commercial': 642,\n",
       " 'sept': 643,\n",
       " 'potential': 644,\n",
       " 'governor': 645,\n",
       " 'working': 646,\n",
       " 'jumped': 647,\n",
       " 'special': 648,\n",
       " 'whose': 649,\n",
       " 'climbed': 650,\n",
       " 'construction': 651,\n",
       " 'meanwhile': 652,\n",
       " 'closing': 653,\n",
       " 'carter': 654,\n",
       " 'never': 655,\n",
       " 'student': 656,\n",
       " 'statistic': 657,\n",
       " 'chicago': 658,\n",
       " 'something': 659,\n",
       " 'started': 660,\n",
       " 'dividend': 661,\n",
       " 'ford': 662,\n",
       " 'california': 663,\n",
       " 'prime': 664,\n",
       " 'attack': 665,\n",
       " 'turned': 666,\n",
       " 'beginning': 667,\n",
       " 'agreed': 668,\n",
       " 'behind': 669,\n",
       " 'source': 670,\n",
       " 'morgan': 671,\n",
       " 'debate': 672,\n",
       " 'virginia': 673,\n",
       " 'really': 674,\n",
       " 'rest': 675,\n",
       " 'legislation': 676,\n",
       " 'jump': 677,\n",
       " 'paid': 678,\n",
       " 'george': 679,\n",
       " 'retirement': 680,\n",
       " 'trying': 681,\n",
       " 'predicted': 682,\n",
       " 'modest': 683,\n",
       " 'advanced': 684,\n",
       " 'german': 685,\n",
       " 'single': 686,\n",
       " 'cutting': 687,\n",
       " 'morning': 688,\n",
       " 'cause': 689,\n",
       " 'south': 690,\n",
       " 'drug': 691,\n",
       " 'kind': 692,\n",
       " 'borrowing': 693,\n",
       " 'posted': 694,\n",
       " 'signal': 695,\n",
       " 'holiday': 696,\n",
       " 'went': 697,\n",
       " 'straight': 698,\n",
       " 'huge': 699,\n",
       " 'trust': 700,\n",
       " 'journal': 701,\n",
       " 'producer': 702,\n",
       " 'done': 703,\n",
       " 'employer': 704,\n",
       " 'oct': 705,\n",
       " 'minute': 706,\n",
       " 'court': 707,\n",
       " 'worst': 708,\n",
       " 'officer': 709,\n",
       " 'different': 710,\n",
       " 'response': 711,\n",
       " 'utility': 712,\n",
       " 'maryland': 713,\n",
       " 'project': 714,\n",
       " 'airline': 715,\n",
       " 'asia': 716,\n",
       " 'changed': 717,\n",
       " 'slowing': 718,\n",
       " 'dealer': 719,\n",
       " 'package': 720,\n",
       " 'moved': 721,\n",
       " 'payroll': 722,\n",
       " 'gap': 723,\n",
       " 'add': 724,\n",
       " 'broker': 725,\n",
       " 'steady': 726,\n",
       " 'improvement': 727,\n",
       " 'largely': 728,\n",
       " 'active': 729,\n",
       " 'slump': 730,\n",
       " 'process': 731,\n",
       " 'able': 732,\n",
       " 'property': 733,\n",
       " 'quickly': 734,\n",
       " 'additional': 735,\n",
       " 'increasing': 736,\n",
       " 'manufacturer': 737,\n",
       " 'grew': 738,\n",
       " 'crash': 739,\n",
       " 'speech': 740,\n",
       " 'particularly': 741,\n",
       " 'sen': 742,\n",
       " 'book': 743,\n",
       " 'declining': 744,\n",
       " 'boom': 745,\n",
       " 'followed': 746,\n",
       " 'plant': 747,\n",
       " 'gdp': 748,\n",
       " 'shift': 749,\n",
       " 'break': 750,\n",
       " 'moving': 751,\n",
       " 'bit': 752,\n",
       " 'stimulus': 753,\n",
       " 'deposit': 754,\n",
       " 'sent': 755,\n",
       " 'serious': 756,\n",
       " 'bid': 757,\n",
       " 'try': 758,\n",
       " 'basis': 759,\n",
       " 'eight': 760,\n",
       " 'unchanged': 761,\n",
       " 'surplus': 762,\n",
       " 'continuing': 763,\n",
       " 'thought': 764,\n",
       " 'grow': 765,\n",
       " 'tech': 766,\n",
       " 'asked': 767,\n",
       " 'let': 768,\n",
       " 'available': 769,\n",
       " 'certain': 770,\n",
       " 'showing': 771,\n",
       " 'william': 772,\n",
       " 'germany': 773,\n",
       " 'worth': 774,\n",
       " 'similar': 775,\n",
       " 'focus': 776,\n",
       " 'relatively': 777,\n",
       " 'expert': 778,\n",
       " 'internet': 779,\n",
       " 'gas': 780,\n",
       " 'greater': 781,\n",
       " 'asian': 782,\n",
       " 'child': 783,\n",
       " 'word': 784,\n",
       " 'stronger': 785,\n",
       " 'weakness': 786,\n",
       " 'strategist': 787,\n",
       " 'pushed': 788,\n",
       " 'running': 789,\n",
       " 'common': 790,\n",
       " 'failed': 791,\n",
       " 'factory': 792,\n",
       " 'needed': 793,\n",
       " 'david': 794,\n",
       " 'seem': 795,\n",
       " 'opportunity': 796,\n",
       " 'moderate': 797,\n",
       " 'class': 798,\n",
       " 'role': 799,\n",
       " 'smaller': 800,\n",
       " 'tokyo': 801,\n",
       " 'warned': 802,\n",
       " 'name': 803,\n",
       " 'saying': 804,\n",
       " 'initial': 805,\n",
       " 'gave': 806,\n",
       " 'hurt': 807,\n",
       " 'estimated': 808,\n",
       " 'noted': 809,\n",
       " 'include': 810,\n",
       " 'texas': 811,\n",
       " 'household': 812,\n",
       " 'either': 813,\n",
       " 'military': 814,\n",
       " 'saw': 815,\n",
       " 'larger': 816,\n",
       " 'continues': 817,\n",
       " 'situation': 818,\n",
       " 'peak': 819,\n",
       " 'track': 820,\n",
       " 'bear': 821,\n",
       " 'light': 822,\n",
       " 'goal': 823,\n",
       " 'staff': 824,\n",
       " 'issued': 825,\n",
       " 'candidate': 826,\n",
       " 'education': 827,\n",
       " 'jan': 828,\n",
       " 'rail': 829,\n",
       " 'borrower': 830,\n",
       " 'revised': 831,\n",
       " 'using': 832,\n",
       " 'feel': 833,\n",
       " 'bring': 834,\n",
       " 'answer': 835,\n",
       " 'woman': 836,\n",
       " 'author': 837,\n",
       " 'drive': 838,\n",
       " 'benchmark': 839,\n",
       " 'analysis': 840,\n",
       " 'approved': 841,\n",
       " 'paper': 842,\n",
       " 'main': 843,\n",
       " 'expects': 844,\n",
       " 'paul': 845,\n",
       " 'volcker': 846,\n",
       " 'trouble': 847,\n",
       " 'consider': 848,\n",
       " 'series': 849,\n",
       " 'fuel': 850,\n",
       " 'size': 851,\n",
       " 'event': 852,\n",
       " 'faster': 853,\n",
       " 'nothing': 854,\n",
       " 'check': 855,\n",
       " 'majority': 856,\n",
       " 'th': 857,\n",
       " 'ap': 858,\n",
       " 'afternoon': 859,\n",
       " 'bureau': 860,\n",
       " 'spring': 861,\n",
       " 'fight': 862,\n",
       " 'chance': 863,\n",
       " 'upward': 864,\n",
       " 'related': 865,\n",
       " 'fallen': 866,\n",
       " 'review': 867,\n",
       " 'bet': 868,\n",
       " 'thus': 869,\n",
       " 'sure': 870,\n",
       " 'positive': 871,\n",
       " 'season': 872,\n",
       " 'accounting': 873,\n",
       " 'remained': 874,\n",
       " 'seek': 875,\n",
       " 'rebound': 876,\n",
       " 'presidential': 877,\n",
       " 'double': 878,\n",
       " 'paying': 879,\n",
       " 'tell': 880,\n",
       " 'organization': 881,\n",
       " 'giant': 882,\n",
       " 'reduced': 883,\n",
       " 'economics': 884,\n",
       " 'present': 885,\n",
       " 'offered': 886,\n",
       " 'weekend': 887,\n",
       " 'difficult': 888,\n",
       " 'reading': 889,\n",
       " 'whole': 890,\n",
       " 'investing': 891,\n",
       " 'actually': 892,\n",
       " 'pension': 893,\n",
       " 'page': 894,\n",
       " 'fast': 895,\n",
       " 'equipment': 896,\n",
       " 'regional': 897,\n",
       " 'negative': 898,\n",
       " 'worse': 899,\n",
       " 'ground': 900,\n",
       " 'london': 901,\n",
       " 'attention': 902,\n",
       " 'uncertainty': 903,\n",
       " 'direction': 904,\n",
       " 'man': 905,\n",
       " 'spokesman': 906,\n",
       " 'margin': 907,\n",
       " 'downturn': 908,\n",
       " 'financing': 909,\n",
       " 'stay': 910,\n",
       " 'minister': 911,\n",
       " 'appears': 912,\n",
       " 'always': 913,\n",
       " 'spend': 914,\n",
       " 'canadian': 915,\n",
       " 'easing': 916,\n",
       " 'conservative': 917,\n",
       " 'caused': 918,\n",
       " 'slide': 919,\n",
       " 'opening': 920,\n",
       " 'bankruptcy': 921,\n",
       " 'broader': 922,\n",
       " 'beyond': 923,\n",
       " 'attempt': 924,\n",
       " 'operating': 925,\n",
       " 'mexico': 926,\n",
       " 'suggests': 927,\n",
       " 'increasingly': 928,\n",
       " 'received': 929,\n",
       " 'warning': 930,\n",
       " 'allow': 931,\n",
       " 'everything': 932,\n",
       " 'hedge': 933,\n",
       " 'perhaps': 934,\n",
       " 'suggest': 935,\n",
       " 'finished': 936,\n",
       " 'client': 937,\n",
       " 'nixon': 938,\n",
       " 'canada': 939,\n",
       " 'james': 940,\n",
       " 'voter': 941,\n",
       " 'mostly': 942,\n",
       " 'chinese': 943,\n",
       " 'night': 944,\n",
       " 'flat': 945,\n",
       " 'interview': 946,\n",
       " 'sense': 947,\n",
       " 'corporation': 948,\n",
       " 'age': 949,\n",
       " 'inventory': 950,\n",
       " 'reach': 951,\n",
       " 'cap': 952,\n",
       " 'closely': 953,\n",
       " 'partner': 954,\n",
       " 'soared': 955,\n",
       " 'easy': 956,\n",
       " 'weekly': 957,\n",
       " 'dec': 958,\n",
       " 'taxpayer': 959,\n",
       " 'practice': 960,\n",
       " 'story': 961,\n",
       " 'suggested': 962,\n",
       " 'lawmaker': 963,\n",
       " 'associated': 964,\n",
       " 'brought': 965,\n",
       " 'quarterly': 966,\n",
       " 'boston': 967,\n",
       " 'person': 968,\n",
       " 'emerging': 969,\n",
       " 'men': 970,\n",
       " 'rating': 971,\n",
       " 'leave': 972,\n",
       " 'picture': 973,\n",
       " 'farm': 974,\n",
       " 'quality': 975,\n",
       " 'passed': 976,\n",
       " 'inflationary': 977,\n",
       " 'climb': 978,\n",
       " 'wide': 979,\n",
       " 'difference': 980,\n",
       " 'transaction': 981,\n",
       " 'addition': 982,\n",
       " 'widely': 983,\n",
       " 'pushing': 984,\n",
       " 'north': 985,\n",
       " 'create': 986,\n",
       " 'save': 987,\n",
       " 'indeed': 988,\n",
       " 'outside': 989,\n",
       " 'starting': 990,\n",
       " 'competition': 991,\n",
       " 'aug': 992,\n",
       " 'anti': 993,\n",
       " 'nine': 994,\n",
       " 'brokerage': 995,\n",
       " 'crude': 996,\n",
       " 'room': 997,\n",
       " 'release': 998,\n",
       " 'kept': 999,\n",
       " 'expense': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8bc0b15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8bc0b15",
    "outputId": "bc0c8ab2-9e8f-470d-a17e-89b96cd00afd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36554"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c30d7",
   "metadata": {
    "id": "016c30d7"
   },
   "source": [
    "### Padding Sequences\n",
    "Sentences and sequences tend to have different lengths, however our model is expecting equally sized observations.\n",
    "Here we want to convert our texts to sequences and make them of the same length (in general, the lenght of the longest of our sequences). We are going to use here `pad_sequences` from Tensorflow (documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences)), to add zeroes to the tokenized sentences until they all reach the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea518a6d",
   "metadata": {
    "id": "ea518a6d"
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(wtxt_train)\n",
    "padded_sequences = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a3a19b1",
   "metadata": {
    "id": "6a3a19b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[186,\n",
       " 2146,\n",
       " 88,\n",
       " 346,\n",
       " 19,\n",
       " 6,\n",
       " 33,\n",
       " 186,\n",
       " 2559,\n",
       " 754,\n",
       " 886,\n",
       " 147,\n",
       " 10,\n",
       " 373,\n",
       " 2943,\n",
       " 370,\n",
       " 25,\n",
       " 346,\n",
       " 19,\n",
       " 1464,\n",
       " 508,\n",
       " 86,\n",
       " 156,\n",
       " 75,\n",
       " 13,\n",
       " 4,\n",
       " 193,\n",
       " 7949,\n",
       " 63,\n",
       " 2146,\n",
       " 565,\n",
       " 1687,\n",
       " 10,\n",
       " 44,\n",
       " 186,\n",
       " 293,\n",
       " 16,\n",
       " 754,\n",
       " 88,\n",
       " 19,\n",
       " 321,\n",
       " 39,\n",
       " 133,\n",
       " 10,\n",
       " 349,\n",
       " 9914,\n",
       " 54,\n",
       " 2,\n",
       " 7950,\n",
       " 3269,\n",
       " 631,\n",
       " 128,\n",
       " 94,\n",
       " 16,\n",
       " 63,\n",
       " 754,\n",
       " 44,\n",
       " 186,\n",
       " 1935,\n",
       " 19,\n",
       " 133,\n",
       " 9914,\n",
       " 46,\n",
       " 10,\n",
       " 9914,\n",
       " 349,\n",
       " 3807,\n",
       " 6,\n",
       " 33,\n",
       " 9114,\n",
       " 2470,\n",
       " 879,\n",
       " 139,\n",
       " 21385,\n",
       " 193,\n",
       " 7949,\n",
       " 2146,\n",
       " 86,\n",
       " 1234,\n",
       " 800,\n",
       " 247,\n",
       " 1,\n",
       " 63,\n",
       " 2146,\n",
       " 1379,\n",
       " 9914,\n",
       " 3,\n",
       " 186,\n",
       " 94,\n",
       " 16,\n",
       " 293,\n",
       " 16,\n",
       " 77,\n",
       " 146,\n",
       " 565,\n",
       " 289,\n",
       " 1110,\n",
       " 2442,\n",
       " 1335,\n",
       " 370,\n",
       " 25,\n",
       " 381,\n",
       " 19,\n",
       " 2615]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf5a3374",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf5a3374",
    "outputId": "99253544-bbca-48ca-e1d5-b86689138957"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  381,   19, 2615],\n",
       "       [   0,    0,    0, ...,  157,   49,  178],\n",
       "       [   0,    0,    0, ...,   10,   83,   43],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  514,  527,  120],\n",
       "       [   0,    0,    0, ...,  278,  103,   59],\n",
       "       [   0,    0,    0, ...,   41,  184,   22]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89dd0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pad_seq'] = padded_sequences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f444fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>pad_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     relevance                                            pad_seq\n",
       "0            1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1            0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2            0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3            0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4            1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "...        ...                                                ...\n",
       "7995         1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7996         0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7997         0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7998         0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "7999         0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['whole_txt'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "561a6b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>whole_txt</th>\n",
       "      <th>pad_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[yield, cd, fell, latest, week, new, york, yie...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[morning, brief, white, house, seek, limit, ch...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[banking, bill, negotiator, set, compromise, p...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[manager, journal, sniffing, drug, abuser, qui...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[currency, trading, dollar, remains, tight, ra...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relevance                                          whole_txt  \\\n",
       "0         1  [yield, cd, fell, latest, week, new, york, yie...   \n",
       "1         0  [morning, brief, white, house, seek, limit, ch...   \n",
       "2         0  [banking, bill, negotiator, set, compromise, p...   \n",
       "3         0  [manager, journal, sniffing, drug, abuser, qui...   \n",
       "4         1  [currency, trading, dollar, remains, tight, ra...   \n",
       "\n",
       "                                             pad_seq  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83826301",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f00e30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_sequences\n",
    "y = df['relevance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fB_vM2GQkf-Y",
   "metadata": {
    "id": "fB_vM2GQkf-Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19ab3a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,    71,   111,    38],\n",
       "       [    0,     0,     0, ...,  2256,   278,  1288],\n",
       "       [    0,     0,     0, ...,   206,    22,    57],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   190,    84,   426],\n",
       "       [    0,     0,     0, ..., 11382,  5063,   149],\n",
       "       [    0,     0,     0, ...,    27,  2800,  2736]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d51f8bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 475)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f8059d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "128bd1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f0eab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "35c078ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe8e8b2",
   "metadata": {
    "id": "0fe8e8b2"
   },
   "source": [
    "### Building the model\n",
    "\n",
    "We are going to build a simple model that includes:\n",
    "- `Embedding` layer with an output representation of each word as a vector of dim 16\n",
    "- `LSTM` (see class slides for more detail or RNNs example notebook for more details) with an intermediate state of 100\n",
    "- An output layer `Dense` that connects the output of the LSTM and creates an output of 3 positions (one per class) as output of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223fed7",
   "metadata": {},
   "source": [
    "That is model nr.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84e1fd75",
   "metadata": {
    "id": "84e1fd75"
   },
   "outputs": [],
   "source": [
    "# We are going to build our model with the Sequential API\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words,      # number of words to process as input\n",
    "                    100,    # output representation\n",
    "                    input_length=len(padded_sequences[0])))    # total length of each observation\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Change activation based on the number of classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "286e1067",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "286e1067",
    "outputId": "ef815e79-2cdb-4372-f0f2-a45738a6bd09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8000, 1), dtype=float32, numpy=\n",
       "array([[0.49995384],\n",
       "       [0.49540925],\n",
       "       [0.50199515],\n",
       "       ...,\n",
       "       [0.502827  ],\n",
       "       [0.50224346],\n",
       "       [0.5016381 ]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "290f727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 475, 100)          3655400   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3735901 (14.25 MB)\n",
      "Trainable params: 3735901 (14.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101ccfc",
   "metadata": {
    "id": "1101ccfc"
   },
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55429b96",
   "metadata": {},
   "source": [
    "### MODEL 1 (The base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "826df8ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "826df8ad",
    "outputId": "954999b9-e6f3-43a2-dc81-a5c10a8e6ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 59s 289ms/step - loss: 0.4641 - accuracy: 0.8203\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.3314 - accuracy: 0.8564\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 57s 284ms/step - loss: 0.1843 - accuracy: 0.9312\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 56s 281ms/step - loss: 0.0919 - accuracy: 0.9648\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 57s 283ms/step - loss: 0.0443 - accuracy: 0.9867\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 58s 288ms/step - loss: 0.0251 - accuracy: 0.9923\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 57s 285ms/step - loss: 0.0145 - accuracy: 0.9956\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 57s 283ms/step - loss: 0.0115 - accuracy: 0.9972\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 57s 285ms/step - loss: 0.0122 - accuracy: 0.9967\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 56s 279ms/step - loss: 0.0069 - accuracy: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16f9b57f880>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1484a19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1484a19",
    "outputId": "5db7ef87-e57e-4f8c-c74b-f124fab03672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(model(padded_sequences).numpy().argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781be31e",
   "metadata": {},
   "source": [
    "Model 1 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76c6df0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 79ms/step - loss: 1.4123 - accuracy: 0.7544\n",
      "Test Loss: 1.4123\n",
      "Test Accuracy: 75.44%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3526a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 77ms/step\n"
     ]
    }
   ],
   "source": [
    "#Prection and Confusion Matrix\n",
    "y_pred = model.predict(X_test)\n",
    "bin_y_pred = (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "062c9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_y_pred = np.squeeze(bin_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "23edf0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f682ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e345456a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1137,  177],\n",
       "       [ 216,   70]], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, bin_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5437a9f",
   "metadata": {},
   "source": [
    "### MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e790349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to build our model with the Sequential API\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Embedding(total_words,      # number of words to process as input\n",
    "                    50,    # output representation\n",
    "                    input_length=len(padded_sequences[0])))    # total length of each observation\n",
    "\n",
    "model2.add(LSTM(50, return_sequences=False))\n",
    "\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a4fded32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 475, 50)           1827700   \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1847951 (7.05 MB)\n",
      "Trainable params: 1847951 (7.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "db80d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2cc28f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 25s 120ms/step - loss: 0.4764 - accuracy: 0.8195 - val_loss: 0.4600 - val_accuracy: 0.8138\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 24s 118ms/step - loss: 0.3539 - accuracy: 0.8430 - val_loss: 0.4664 - val_accuracy: 0.7819\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 24s 118ms/step - loss: 0.2114 - accuracy: 0.9150 - val_loss: 0.5842 - val_accuracy: 0.7800\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 24s 119ms/step - loss: 0.1117 - accuracy: 0.9613 - val_loss: 0.6577 - val_accuracy: 0.7406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16fa0208040>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, epochs=5, validation_data = (X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a59b099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 2s 46ms/step - loss: 0.4600 - accuracy: 0.8138\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880874d",
   "metadata": {
    "id": "8880874d"
   },
   "source": [
    "Adjust the model architecture and hyperparameters based on your specific problem and data.\n",
    "\n",
    "This is a basic example to get you started with implementing an RNN for text classification using TensorFlow. Fine-tune and expand upon this foundation according to your project requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f1b11",
   "metadata": {
    "id": "218f1b11"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba69cf",
   "metadata": {
    "id": "7fba69cf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44b5f5a5",
   "metadata": {
    "id": "44b5f5a5"
   },
   "source": [
    "## Implementing an RNN for Text Classification with TensorFlow\n",
    "\n",
    "In this example, we'll build a simple Recurrent Neural Network (RNN) using TensorFlow for text classification. The dataset consists of one or two sentences as input data.\n",
    "\n",
    "This is a simplified and applied version with focus on the usage of Tensorflow. For a more detailed and extensive process description please refer to the \"RNNs example\" notebook or to the class presentations\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
